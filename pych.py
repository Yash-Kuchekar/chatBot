import streamlit as st

from PyPDF2 import PdfReader

from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings

GROQ_API_KEY = "Your API Key"

st.header("PDF Analyser")

with st.sidebar:
    st.title("My Notes"),
    file = st.file_uploader("Upload notes PDF and start asking questions", type="pdf")

# extracting the text from pdf file
if file is not None:
    my_pdf = PdfReader(file)
    text = ""
    for page in my_pdf.pages:
        text += page.extract_text() or ""  # safer extraction

    # break it into Chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50, length_function=len)
    chunks = splitter.split_text(text)

    # Use HuggingFaceEmbeddings for embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"  # You can change to any supported model
    )

    # Creating VectorDB & Storing embeddings into it
    vector_store = FAISS.from_texts(chunks, embeddings)

    # get user query
    user_query = st.text_input("Type your query here")

    # semantic search from vector store
    if user_query:
        matching_chunks = vector_store.similarity_search(user_query)

        # define our LLM using Groq
        # ...existing code...
        # define our LLM using Groq
        llm = ChatGroq(
            api_key=GROQ_API_KEY,
            model="llama3-8b-8192",
            # max_tokens=1000,
            temperature=0.6
        )

        customized_prompt = ChatPromptTemplate.from_template(
            """You are my assistant tutor. Answer the question based on the following context and if you did not get the context simply say "I don't know Aditya" : {context} Question: {input} """
        )

        chain = create_stuff_documents_chain(llm, customized_prompt)
        output = chain.invoke({"input": user_query, "context": matching_chunks})

        st.write(output)